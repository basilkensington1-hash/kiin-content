#!/usr/bin/env python3
"""
Caregiver Tips Video Generator V2 - Professional Educational Content
10x enhanced version with animations, professional design, brand integration

New Features:
- Animated icons and transitions
- Background music integration
- Data visualizations 
- Professional educational structure
- Brand-consistent design
- Voice management integration
- Progress indicators
- Memory aids and takeaways
- Series organization

Requirements:
- All V1 requirements plus:
- moviepy: pip install moviepy
- matplotlib: pip install matplotlib  
- requests: pip install requests (for downloading assets)
"""

import json
import os
import asyncio
import tempfile
import shutil
import random
import re
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from PIL import Image, ImageDraw, ImageFont, ImageFilter, ImageEnhance
import edge_tts
import subprocess
try:
    from moviepy.editor import *
    from moviepy.config import check_and_download_cmd
    MOVIEPY_AVAILABLE = True
except ImportError:
    print("MoviePy not available, falling back to FFmpeg for video creation")
    MOVIEPY_AVAILABLE = False
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.patches import Circle
import numpy as np

# Import our enhanced modules
from brand_utils import KiinBrand, get_brand_voice, get_brand_tone
from voice_manager import VoiceManager
from effects import KiinEffectsLibrary

# Enhanced Constants
VIDEO_WIDTH = 1080
VIDEO_HEIGHT = 1920
FPS = 30

# Timing (in seconds) - Extended for professional content
INTRO_DURATION = 4
HOOK_DURATION = 5
PROBLEM_DURATION = 12
SOLUTION_DURATION = 15
TAKEAWAY_DURATION = 8
ACTION_DURATION = 6
OUTRO_DURATION = 4
TOTAL_DURATION = INTRO_DURATION + HOOK_DURATION + PROBLEM_DURATION + SOLUTION_DURATION + TAKEAWAY_DURATION + ACTION_DURATION + OUTRO_DURATION

class TipsGeneratorV2:
    def __init__(self, config_path: str, output_dir: str):
        self.config_path = Path(config_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Initialize brand and voice management
        self.brand = KiinBrand()
        self.voice_manager = VoiceManager()
        
        # Initialize professional effects library
        self.effects = KiinEffectsLibrary(VIDEO_WIDTH, VIDEO_HEIGHT, FPS)
        
        # Load enhanced tips data
        with open(self.config_path, 'r') as f:
            self.data = json.load(f)
        
        # Enhanced color palette from brand
        self.colors = {
            'primary': self.brand.get_color_rgb('primary'),      # Trustworthy blue
            'secondary': self.brand.get_color_rgb('secondary'),  # Gentle green
            'accent': self.brand.get_color_rgb('accent'),        # Warm orange
            'background': self.brand.get_color_rgb('background_warm'),
            'text_dark': self.brand.get_color_rgb('text_dark'),
            'text_light': self.brand.get_color_rgb('text_light'),
            'danger': (220, 53, 69),     # For "wrong" sections
            'success': (25, 135, 84),    # For "right" sections
            'warning': (255, 193, 7),    # For "caution" elements
            'info': (31, 81, 255)        # For informational elements
        }
        
        # Voice configuration
        self.voice_config = self.brand.get_voice_config()
        self.current_voice = self.voice_config.get('tts_voice', 'en-US-AriaNeural')
        
        # Animation settings
        self.animation_settings = {
            'fade_duration': 0.5,
            'slide_duration': 0.8,
            'bounce_height': 50,
            'rotation_angle': 15,
            'scale_factor': 1.2
        }
        
        # Music and sound effects paths
        self.audio_assets = {
            'background_music': self._get_background_music_path(),
            'transition_sound': self._get_transition_sound_path(),
            'success_sound': self._get_success_sound_path(),
            'intro_music': self._get_intro_music_path()
        }
    
    def _get_background_music_path(self) -> Optional[str]:
        """Get background music path based on content mood"""
        music_dir = Path(__file__).parent.parent / 'assets' / 'music' / 'professional'
        if music_dir.exists():
            music_files = list(music_dir.glob('*.mp3')) + list(music_dir.glob('*.wav'))
            if music_files:
                return str(random.choice(music_files))
        return None
    
    def _get_transition_sound_path(self) -> Optional[str]:
        """Get transition sound effect path"""
        # For now, we'll generate this programmatically or use silence
        # In production, you'd have actual sound effect files
        return None
    
    def _get_success_sound_path(self) -> Optional[str]:
        """Get success sound effect path"""
        return None
    
    def _get_intro_music_path(self) -> Optional[str]:
        """Get intro music path"""
        music_dir = Path(__file__).parent.parent / 'assets' / 'music' / 'warm'
        if music_dir.exists():
            music_files = list(music_dir.glob('*.mp3')) + list(music_dir.glob('*.wav'))
            if music_files:
                return str(random.choice(music_files))
        return None

    def get_enhanced_font(self, size: int = 60, weight: str = 'normal') -> ImageFont.FreeTypeFont:
        """Get enhanced font with brand consistency"""
        try:
            # Try to use brand fonts
            font_paths = [
                f"/System/Library/Fonts/{self.brand.fonts['font_stack']['heading']['primary']}.ttc",
                "/System/Library/Fonts/Helvetica.ttc",
                "/System/Library/Fonts/Arial.ttf",
                "/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf",
            ]
            
            for font_path in font_paths:
                if os.path.exists(font_path):
                    return ImageFont.truetype(font_path, size)
            
            return ImageFont.load_default()
        except:
            return ImageFont.load_default()

    def create_gradient_background(self, width: int, height: int, 
                                 color1: Tuple[int, int, int], 
                                 color2: Tuple[int, int, int],
                                 direction: str = 'vertical') -> Image.Image:
        """Create a gradient background"""
        base = Image.new('RGB', (width, height), color1)
        top = Image.new('RGB', (width, height), color2)
        
        if direction == 'vertical':
            gradient = Image.new('L', (1, height))
            for y in range(height):
                gradient.putpixel((0, y), int(255 * (y / height)))
            alpha = gradient.resize((width, height))
        else:  # horizontal
            gradient = Image.new('L', (width, 1))
            for x in range(width):
                gradient.putpixel((x, 0), int(255 * (x / width)))
            alpha = gradient.resize((width, height))
        
        top.putalpha(alpha)
        base.paste(top, (0, 0), top)
        return base

    def create_animated_icon(self, icon_type: str, color: Tuple[int, int, int], 
                           size: int = 120) -> Image.Image:
        """Create animated icons (checkmark, X, etc.)"""
        image = Image.new('RGBA', (size, size), (0, 0, 0, 0))
        draw = ImageDraw.Draw(image)
        center = size // 2
        
        if icon_type == 'checkmark':
            # Draw animated checkmark
            points = [
                (center - 20, center),
                (center - 5, center + 15),
                (center + 20, center - 15)
            ]
            draw.polygon(points, fill=color)
            
        elif icon_type == 'x_mark':
            # Draw animated X
            line_width = 8
            draw.line([(center-20, center-20), (center+20, center+20)], fill=color, width=line_width)
            draw.line([(center-20, center+20), (center+20, center-20)], fill=color, width=line_width)
            
        elif icon_type == 'lightbulb':
            # Draw lightbulb for insights
            # Bulb
            draw.ellipse([(center-15, center-25), (center+15, center+5)], fill=color)
            # Base
            draw.rectangle([(center-10, center+5), (center+10, center+15)], fill=color)
            # Lines for light effect
            for i in range(3):
                y_offset = center - 35 - (i * 8)
                draw.line([(center-5, y_offset), (center+5, y_offset)], fill=color, width=3)
                
        elif icon_type == 'heart':
            # Draw heart for emotional content
            draw.ellipse([(center-15, center-10), (center, center+5)], fill=color)
            draw.ellipse([(center, center-10), (center+15, center+5)], fill=color)
            draw.polygon([(center-15, center), (center, center+20), (center+15, center)], fill=color)
            
        elif icon_type == 'brain':
            # Draw brain for memory aids
            draw.ellipse([(center-20, center-15), (center+20, center+15)], fill=color)
            # Add texture lines
            for i in range(5):
                y = center - 10 + (i * 5)
                draw.line([(center-15, y), (center+15, y)], fill=self.colors['background'], width=2)
        
        return image

    def create_progress_indicator(self, current: int, total: int, width: int = 300) -> Image.Image:
        """Create progress indicator for video sections"""
        height = 20
        image = Image.new('RGBA', (width, height), (0, 0, 0, 0))
        draw = ImageDraw.Draw(image)
        
        # Background bar
        draw.rounded_rectangle([0, 0, width-1, height-1], radius=10, 
                             fill=(*self.colors['background'], 100))
        
        # Progress bar
        progress_width = int((current / total) * width)
        if progress_width > 0:
            draw.rounded_rectangle([0, 0, progress_width, height-1], radius=10,
                                 fill=self.colors['primary'])
        
        return image

    def create_data_visualization(self, data_point: str, tip_data: Dict) -> Image.Image:
        """Create simple data visualization for statistics"""
        fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
        fig.patch.set_facecolor('white')
        
        # Extract percentage from data_point if present
        import re
        percentage_match = re.search(r'(\d+)%', data_point)
        if percentage_match:
            percentage = int(percentage_match.group(1))
            
            # Create a simple bar chart
            categories = ['Before Tip', 'After Tip']
            values = [100 - percentage, 100]  # Show improvement
            
            # Convert RGB values to 0-1 range for matplotlib
            danger_color = tuple(c/255 for c in self.colors['danger'][:3])
            success_color = tuple(c/255 for c in self.colors['success'][:3])
            bars = ax.bar(categories, values, color=[danger_color, success_color])
            ax.set_ylabel('Effectiveness %')
            ax.set_title('Impact of This Tip', fontsize=14, fontweight='bold')
            ax.set_ylim(0, 100)
            
            # Add percentage labels
            for bar, value in zip(bars, values):
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height + 1,
                       f'{int(value)}%', ha='center', va='bottom', fontweight='bold')
        else:
            # Generic improvement chart
            categories = ['Stress', 'Confidence', 'Connection']
            before = [80, 30, 40]  # High stress, low confidence and connection
            after = [30, 85, 80]   # Low stress, high confidence and connection
            
            x = np.arange(len(categories))
            width = 0.35
            
            danger_color = tuple(c/255 for c in self.colors['danger'][:3])
            success_color = tuple(c/255 for c in self.colors['success'][:3])
            bars1 = ax.bar(x - width/2, before, width, label='Before', color=danger_color)
            bars2 = ax.bar(x + width/2, after, width, label='After', color=success_color)
            
            ax.set_ylabel('Level (0-100)')
            ax.set_title('Caregiving Experience Improvement', fontweight='bold')
            ax.set_xticks(x)
            ax.set_xticklabels(categories)
            ax.legend()
            ax.set_ylim(0, 100)
        
        # Save to PIL Image
        fig.tight_layout()
        temp_path = tempfile.mktemp(suffix='.png')
        plt.savefig(temp_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        image = Image.open(temp_path)
        os.unlink(temp_path)
        
        # Resize to fit video
        image = image.resize((int(VIDEO_WIDTH * 0.8), int(VIDEO_HEIGHT * 0.3)))
        return image

    def create_memory_aid_visual(self, memory_aid: str, width: int = 800) -> Image.Image:
        """Create visual memory aid with icons and text"""
        height = 150
        image = Image.new('RGBA', (width, height), (0, 0, 0, 0))
        draw = ImageDraw.Draw(image)
        
        # Background with rounded corners
        draw.rounded_rectangle([0, 0, width-1, height-1], radius=20,
                             fill=(*self.colors['accent'], 50),
                             outline=self.colors['accent'], width=3)
        
        # Extract emoji if present
        emoji_match = re.search(r'([ğŸ­ğŸ”„ğŸ’¤ğŸ¤—ğŸ›‘â¤ï¸ğŸ’™ğŸŒ¬ï¸ğŸ’â°ğŸ¤ğŸƒâ€â™€ï¸ğŸ†˜ğŸ¨ğŸ‘”ğŸŒŸğŸ”®ğŸ“‹ğŸ’«ğŸŒ…ğŸ’“ğŸ¤±ğŸ‰])', memory_aid)
        emoji = emoji_match.group(1) if emoji_match else 'ğŸ’¡'
        
        # Draw memory aid text
        font_large = self.get_enhanced_font(32, 'bold')
        font_small = self.get_enhanced_font(24)
        
        # Position emoji and text
        text_without_emoji = memory_aid.replace(emoji, '').strip() if emoji_match else memory_aid
        
        # Draw emoji area
        draw.text((30, height//2 - 20), emoji, font=font_large, fill=self.colors['text_dark'])
        
        # Draw main text
        text_x = 100
        draw.text((text_x, height//2 - 15), text_without_emoji, 
                 font=font_small, fill=self.colors['text_dark'])
        
        return image

    def create_section_image_v2(self, section: str, content: Dict, 
                               section_number: int, total_sections: int,
                               frame: int = 15, total_frames: int = 30) -> Image.Image:
        """Create enhanced section image with professional cinematic effects"""
        
        # Section configuration with enhanced styling
        section_config = {
            'intro': {
                'palette': 'kiin_brand',
                'mood': 'professional',
                'icon_type': 'lightbulb',
                'title': 'KIIN CAREGIVER TIP',
                'bg_style': 'cinematic',
                'text_treatment': 'premium',
                'particles': 'professional'
            },
            'hook': {
                'palette': 'cinematic_warm',
                'mood': 'energetic', 
                'icon_type': 'heart_beat',
                'title': 'ATTENTION!',
                'bg_style': 'energetic',
                'text_treatment': 'glow',
                'particles': 'energetic'
            },
            'problem': {
                'palette': 'dramatic',
                'mood': 'dramatic',
                'icon_type': 'x_mark',
                'title': "âŒ AVOID THIS MISTAKE",
                'bg_style': 'flowing',
                'text_treatment': 'dramatic',
                'particles': 'professional'
            },
            'solution': {
                'palette': 'organic',
                'mood': 'warm',
                'icon_type': 'growing_plant',
                'title': 'âœ… BETTER APPROACH',
                'bg_style': 'energetic',
                'text_treatment': 'premium',
                'particles': 'calming'
            },
            'takeaway': {
                'palette': 'cinematic_cool',
                'mood': 'professional',
                'icon_type': 'brain',
                'title': 'ğŸ§  REMEMBER THIS',
                'bg_style': 'flowing',
                'text_treatment': 'glow',
                'particles': 'professional'
            },
            'action': {
                'palette': 'cinematic_warm',
                'mood': 'energetic',
                'icon_type': 'bouncing_ball',
                'title': 'ğŸ¯ TAKE ACTION TODAY',
                'bg_style': 'energetic',
                'text_treatment': 'premium',
                'particles': 'energetic'
            },
            'outro': {
                'palette': 'organic',
                'mood': 'warm',
                'icon_type': 'heart_beat',
                'title': 'YOU\'VE GOT THIS! ğŸ’™',
                'bg_style': 'flowing',
                'text_treatment': 'glow',
                'particles': 'calming'
            }
        }
        
        config = section_config.get(section, section_config['intro'])
        
        # Create professional animated background
        background = self.effects.animated_gradient_pro(
            frame, total_frames, config['palette'], config['bg_style']
        )
        
        # Add premium particle effects
        particles = self.effects.premium_particle_system(
            frame, total_frames, config['particles'], 'medium'
        )
        background = Image.alpha_composite(background, particles)
        
        # Add cinematic bokeh for depth
        if section in ['solution', 'takeaway', 'outro']:
            bokeh = self.effects.cinematic_bokeh(frame, total_frames, 'warm', 0.4)
            background = Image.alpha_composite(background, bokeh)
        
        # Create content dictionary for professional frame generation
        frame_content = {
            'title': config['title'],
            'mood': config['mood']
        }
        
        # Get main content text
        if section in ['hook', 'problem', 'solution']:
            main_text = content.get(section, '')
        elif section == 'takeaway':
            main_text = content.get('key_takeaway', '')
        elif section == 'action':
            main_text = content.get('action_today', '')
        elif section == 'intro':
            difficulty = content.get('difficulty', 'beginner').title()
            category = content.get('category', 'general').title().replace('_', ' ')
            main_text = f"{difficulty} Level â€¢ {category} Series"
        elif section == 'outro':
            main_text = "Every small act of care makes a difference. You're doing amazing work!"
        else:
            main_text = ''
        
        if main_text:
            frame_content['subtitle'] = main_text
        
        # Generate professional video frame
        result = self.effects.create_professional_video_frame(
            frame_content, frame, total_frames, section
        )
        
        # Add progress visualization
        progress_value = section_number / total_sections
        progress_viz = self.effects.progress_visualization(
            frame, total_frames, progress_value, 'modern', 'tip_progress'
        )
        result = Image.alpha_composite(result, progress_viz)
        
        # Add animated icon
        if config['icon_type']:
            icon_animation = self.effects.animated_icons_library(
                frame, total_frames, config['icon_type'], 
                'positive' if section in ['solution', 'action'] else 'neutral'
            )
            result = Image.alpha_composite(result, icon_animation)
        
        # Add cinematic title with professional treatment
        if config['title']:
            title_overlay = self.effects.cinematic_title_reveal(
                config['title'], main_text[:50] + '...' if len(main_text) > 50 else main_text,
                frame, total_frames, 'epic', config['palette']
            )
            result = Image.alpha_composite(result, title_overlay)
        
        # Add data visualization for solution sections
        if section == 'solution' and content.get('data_point'):
            try:
                # Extract percentage or create improvement data
                import re
                percentage_match = re.search(r'(\d+)%', content['data_point'])
                if percentage_match:
                    improvement = int(percentage_match.group(1))
                    data = [30, improvement]  # Before vs After
                else:
                    data = [40, 85, 90]  # Generic improvement story
                
                data_viz = self.effects.data_story_charts(
                    frame, total_frames, data, 'impact', 'improvement'
                )
                result = Image.alpha_composite(result, data_viz)
            except Exception as e:
                print(f"Could not create data visualization: {e}")
        
        # Add call-to-action for action sections
        if section == 'action':
            cta_text = "Try This Today!"
            cta_animation = self.effects.call_to_action_pro(
                frame, total_frames, cta_text, 'medium'
            )
            result = Image.alpha_composite(result, cta_animation)
        
        # Add memory aid visual enhancement
        if section in ['takeaway', 'action'] and content.get('memory_aid'):
            # Create enhanced memory aid with effects
            memory_text = content['memory_aid']
            memory_overlay = self._create_enhanced_memory_aid(
                memory_text, frame, total_frames
            )
            result = Image.alpha_composite(result, memory_overlay)
        
        # Apply final cinematic polish
        result = self.effects.mood_responsive_effects(result, config['mood'], 0.8)
        result = self.effects.adaptive_vignette(result, section)
        result = self.effects.film_grain_texture(result, 'modern')
        
        # Add professional watermark
        try:
            watermark_path = self.brand.get_watermark_path()
            if watermark_path.exists():
                watermark = Image.open(watermark_path).convert('RGBA')
                # Professional watermark treatment
                wm_size = int(VIDEO_WIDTH * 0.04)  # Smaller, more elegant
                watermark = watermark.resize((wm_size, wm_size), Image.Resampling.LANCZOS)
                
                # Apply subtle glow to watermark
                watermark = self.effects.advanced_effects.add_glow_effect(
                    watermark, (255, 255, 255), 0.3, 4
                )
                
                # Position elegantly
                wm_x = VIDEO_WIDTH - wm_size - 40
                wm_y = VIDEO_HEIGHT - wm_size - 40
                result.paste(watermark, (wm_x, wm_y), watermark)
        except Exception as e:
            print(f"Could not add watermark: {e}")
        
        return result

    def _create_enhanced_memory_aid(self, memory_text: str, frame: int, total_frames: int) -> Image.Image:
        """Create enhanced memory aid with cinematic effects"""
        image = Image.new('RGBA', (VIDEO_WIDTH, VIDEO_HEIGHT), (0, 0, 0, 0))
        
        # Create memory aid as subtitle-style overlay
        memory_overlay = self.effects.advanced_effects.subtitle_popup_animation(
            f"ğŸ’¡ {memory_text}", frame, total_frames,
            self.effects._get_professional_font(32, 'medium'),
            (255, 255, 255), (41, 98, 255)  # Kiin brand blue background
        )
        
        # Position in lower third
        memory_y_offset = int(VIDEO_HEIGHT * 0.75)
        paste_img = Image.new('RGBA', (VIDEO_WIDTH, VIDEO_HEIGHT), (0, 0, 0, 0))
        paste_img.paste(memory_overlay, (0, memory_y_offset - VIDEO_HEIGHT // 2))
        
        return paste_img

    def wrap_text_enhanced(self, text: str, font: ImageFont.FreeTypeFont, max_width: int) -> List[str]:
        """Enhanced text wrapping with better word breaking"""
        words = text.split()
        lines = []
        current_line = []
        
        for word in words:
            test_line = current_line + [word]
            test_text = ' '.join(test_line)
            
            bbox = font.getbbox(test_text)
            text_width = bbox[2] - bbox[0]
            
            if text_width <= max_width:
                current_line = test_line
            else:
                if current_line:
                    lines.append(' '.join(current_line))
                    current_line = [word]
                else:
                    # Word too long, break it
                    lines.append(word)
        
        if current_line:
            lines.append(' '.join(current_line))
        
        return lines

    async def generate_enhanced_audio(self, tip: Dict, voice_name: str = None) -> str:
        """Generate enhanced audio with appropriate voice for content type"""
        if voice_name is None:
            voice_name = 'en-US-AriaNeural'  # Use default reliable voice
        
        # Create simplified script
        script_parts = [
            "Here's an essential caregiving tip.",
            f"{tip['hook']}",
            f"Why this matters: {tip['wrong']}",
            f"Instead, try this: {tip['right']}",
            tip['encouragement']
        ]
        
        # Join with periods for natural pauses
        full_script = '. '.join(script_parts) + '.'
        
        # Generate audio
        temp_audio = tempfile.mktemp(suffix='.wav')
        communicate = edge_tts.Communicate(full_script, voice_name)
        await communicate.save(temp_audio)
        
        return temp_audio

    def create_video_with_moviepy(self, sections: List[Tuple[str, Image.Image, float]], 
                                 audio_path: str, output_path: str, tip: Dict):
        """Create video using MoviePy with cinematic frame-by-frame effects"""
        if not MOVIEPY_AVAILABLE:
            print("MoviePy not available, falling back to FFmpeg")
            self.create_video_with_ffmpeg(sections, audio_path, output_path, tip)
            return
        
        try:
            clips = []
            
            print("   ğŸ¬ Creating cinematic video clips with frame-by-frame effects...")
            
            # Generate frame-by-frame animations for each section
            for i, (section_name, base_image, duration) in enumerate(sections):
                print(f"      ğŸ¨ Animating {section_name} section...")
                
                # Calculate frames for this section
                section_frames = int(duration * FPS)
                frame_images = []
                
                # Generate animated frames using our enhanced effects
                for frame in range(section_frames):
                    # Create professional animated frame
                    animated_frame = self.create_section_image_v2(
                        section_name, tip, i+1, len(sections), frame, section_frames
                    )
                    
                    # Save frame temporarily
                    temp_frame_path = tempfile.mktemp(suffix='.png')
                    animated_frame.save(temp_frame_path)
                    frame_images.append(temp_frame_path)
                
                # Create video clip from frame sequence
                if frame_images:
                    # Use ImageSequenceClip for smooth animation
                    clip = ImageSequenceClip(frame_images, fps=FPS, with_mask=False)
                    
                    # Add cinematic transitions between sections
                    if i > 0:
                        # Get transition type based on content flow
                        prev_section = sections[i-1][0]
                        transition_context = self._get_transition_context(prev_section, section_name)
                        
                        # Add crossfade transition
                        clip = clip.crossfadein(0.5)
                    
                    # Add section-specific motion effects
                    clip = self._apply_section_motion_effects(clip, section_name)
                    
                    clips.append(clip)
                
                # Cleanup frame files
                for frame_path in frame_images:
                    if os.path.exists(frame_path):
                        os.unlink(frame_path)
            
            print("   ğŸµ Assembling final video with audio...")
            
            # Concatenate all clips with transitions
            video = concatenate_videoclips(clips, method="compose")
            
            # Load and enhance audio
            audio = AudioFileClip(audio_path)
            
            # Add professional audio processing
            if self.audio_assets['background_music']:
                try:
                    bg_music = AudioFileClip(self.audio_assets['background_music'])
                    # Loop and mix background music
                    if bg_music.duration < video.duration:
                        bg_music = bg_music.loop(duration=video.duration)
                    else:
                        bg_music = bg_music.subclip(0, video.duration)
                    
                    # Professional audio mixing
                    bg_music = bg_music.volumex(0.12)  # Very subtle background
                    final_audio = CompositeAudioClip([audio.volumex(0.88), bg_music])
                    
                    # Add subtle fade in/out
                    final_audio = final_audio.audio_fadein(0.5).audio_fadeout(0.5)
                    video = video.set_audio(final_audio)
                except Exception as e:
                    print(f"Could not add background music: {e}")
                    video = video.set_audio(audio.audio_fadein(0.3).audio_fadeout(0.3))
            else:
                video = video.set_audio(audio.audio_fadein(0.3).audio_fadeout(0.3))
            
            # Write final video with optimal settings
            print("   ğŸ“¼ Rendering final video...")
            video.write_videofile(
                output_path,
                fps=FPS,
                codec='libx264',
                preset='medium',  # Better quality
                audio_codec='aac',
                audio_bitrate='128k',
                temp_audiofile=tempfile.mktemp(suffix='.m4a'),
                remove_temp=True,
                verbose=False,
                logger=None,
                ffmpeg_params=['-crf', '20']  # High quality
            )
            
            # Cleanup
            video.close()
            audio.close()
            
        except Exception as e:
            print(f"MoviePy creation failed, falling back to FFmpeg: {e}")
            import traceback
            traceback.print_exc()
            # Fallback to original FFmpeg method
            self.create_video_with_ffmpeg(sections, audio_path, output_path, tip)

    def create_video_with_ffmpeg(self, sections: List[Tuple[str, Image.Image, float]], 
                               audio_path: str, output_path: str, tip: Dict):
        """Fallback method using FFmpeg"""
        with tempfile.TemporaryDirectory() as temp_dir:
            # Save images
            image_paths = []
            for i, (section_name, image, duration) in enumerate(sections):
                image_path = os.path.join(temp_dir, f"section_{i:02d}.png")
                image.save(image_path)
                image_paths.append((image_path, duration))
            
            # Create filter complex for smooth transitions
            inputs = []
            filters = []
            
            for i, (image_path, duration) in enumerate(image_paths):
                inputs.extend(['-loop', '1', '-t', str(duration), '-i', image_path])
                filters.append(f'[{i}:v]setpts=PTS-STARTPTS[v{i}]')
            
            # Add crossfade transitions
            if len(image_paths) > 1:
                fade_duration = 0.5
                transition_filters = []
                current_filter = '[v0]'
                
                for i in range(1, len(image_paths)):
                    next_filter = f'[v{i}]'
                    output_filter = f'[out{i}]' if i < len(image_paths) - 1 else '[outv]'
                    
                    transition_filters.append(
                        f'{current_filter}{next_filter}xfade=transition=fade:duration={fade_duration}:offset={sum(d for _, d in image_paths[:i]) - fade_duration}{output_filter}'
                    )
                    current_filter = f'[out{i}]' if i < len(image_paths) - 1 else '[outv]'
                
                filters.extend(transition_filters)
            else:
                filters.append('[v0][outv]')
            
            filter_complex = ';'.join(filters)
            
            # Build FFmpeg command
            cmd = ['ffmpeg', '-y'] + inputs + ['-i', audio_path]
            cmd.extend([
                '-filter_complex', filter_complex,
                '-map', '[outv]',
                '-map', f'{len(image_paths)}:a',
                '-c:v', 'libx264',
                '-c:a', 'aac',
                '-r', str(FPS),
                '-pix_fmt', 'yuv420p',
                '-shortest',
                output_path
            ])
            
            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                raise Exception(f"FFmpeg failed: {result.stderr}")

    async def generate_tip_video_v2(self, tip: Dict, output_filename: str = None, 
                                   voice_name: str = None) -> str:
        """Generate enhanced tip video with all new features"""
        if not output_filename:
            series_info = tip.get('series', 'general')
            difficulty = tip.get('difficulty', 'beginner')
            output_filename = f"tip_v2_{tip['id']:02d}_{series_info}_{difficulty}.mp4"
        
        output_path = self.output_dir / output_filename
        
        print(f"ğŸ¬ Generating enhanced video for tip {tip['id']}: {tip['hook'][:50]}...")
        print(f"   ğŸ“Š Difficulty: {tip.get('difficulty', 'beginner').title()}")
        print(f"   ğŸ¯ Series: {tip.get('series', 'general').title()}")
        print(f"   ğŸµ Voice: {voice_name or self.current_voice}")
        
        # Create video sections with enhanced structure
        sections = []
        section_durations = [
            ('intro', INTRO_DURATION),
            ('hook', HOOK_DURATION),
            ('problem', PROBLEM_DURATION),
            ('solution', SOLUTION_DURATION),
            ('takeaway', TAKEAWAY_DURATION),
            ('action', ACTION_DURATION),
            ('outro', OUTRO_DURATION)
        ]
        
        for i, (section_name, duration) in enumerate(section_durations):
            print(f"   ğŸ¨ Creating {section_name} section...")
            image = self.create_section_image_v2(section_name, tip, i+1, len(section_durations))
            sections.append((section_name, image, duration))
        
        # Generate enhanced audio
        print("   ğŸ¤ Generating professional audio...")
        audio_path = await self.generate_enhanced_audio(tip, voice_name)
        
        try:
            # Create video with MoviePy for best quality
            print("   ğŸï¸  Assembling video with transitions...")
            self.create_video_with_moviepy(sections, audio_path, str(output_path), tip)
            
            print(f"âœ… Enhanced video generated: {output_path}")
            
            # Add metadata
            file_size = os.path.getsize(output_path) / (1024 * 1024)  # MB
            print(f"   ğŸ“ File size: {file_size:.2f} MB")
            print(f"   ğŸ“ Resolution: {VIDEO_WIDTH}x{VIDEO_HEIGHT} (9:16)")
            print(f"   â±ï¸  Duration: ~{TOTAL_DURATION} seconds")
            print(f"   ğŸ¯ Features: Animations, Transitions, Data Viz, Brand Integration")
            
            return str(output_path)
            
        finally:
            # Cleanup temp audio
            if os.path.exists(audio_path):
                os.unlink(audio_path)

    def generate_series_videos(self, series_name: str) -> List[str]:
        """Generate all videos for a specific series"""
        series_info = None
        for series in self.data['metadata']['series']:
            if series['name'] == series_name:
                series_info = series
                break
        
        if not series_info:
            raise ValueError(f"Series '{series_name}' not found")
        
        print(f"ğŸ¬ Generating {series_info['title']} series...")
        print(f"   ğŸ“ {series_info['description']}")
        print(f"   ğŸ“Š {len(series_info['tip_ids'])} videos in series")
        
        generated_videos = []
        
        for i, tip_id in enumerate(series_info['tip_ids']):
            tip = next((t for t in self.data['tips'] if t['id'] == tip_id), None)
            if tip:
                print(f"\nğŸ“¹ Creating video {i+1}/{len(series_info['tip_ids'])}")
                filename = f"{series_name}_day_{i+1:02d}_{tip['id']:02d}.mp4"
                video_path = asyncio.run(self.generate_tip_video_v2(tip, filename))
                generated_videos.append(video_path)
        
        print(f"\nğŸ‰ Series complete! Generated {len(generated_videos)} videos")
        return generated_videos

    def get_tips_by_difficulty(self, difficulty: str) -> List[Dict]:
        """Get tips filtered by difficulty level"""
        return [tip for tip in self.data['tips'] if tip.get('difficulty') == difficulty]

    def _get_transition_context(self, from_section: str, to_section: str) -> str:
        """Determine transition context based on section flow"""
        context_map = {
            ('intro', 'hook'): 'neutral',
            ('hook', 'problem'): 'insight_reveal',
            ('problem', 'solution'): 'problem_to_solution',
            ('solution', 'takeaway'): 'emotional_shift',
            ('takeaway', 'action'): 'neutral',
            ('action', 'outro'): 'emotional_shift'
        }
        
        return context_map.get((from_section, to_section), 'neutral')
    
    def _apply_section_motion_effects(self, clip, section_name: str):
        """Apply motion effects based on section type"""
        
        if section_name in ['hook', 'problem']:
            # Attention-grabbing entrance
            return clip.resize(lambda t: min(1.0, 0.8 + 0.2 * t))
            
        elif section_name in ['solution', 'takeaway']:
            # Gentle, confident motion
            return clip.set_position(lambda t: (0, -5 * np.sin(t * 2)))
            
        elif section_name == 'action':
            # Energetic, motivating motion
            return clip.resize(lambda t: 1.0 + 0.02 * np.sin(t * 4))
            
        else:
            # Standard, stable presentation
            return clip

    def get_tips_by_series(self, series_name: str) -> List[Dict]:
        """Get tips for a specific series"""
        series_info = next((s for s in self.data['metadata']['series'] if s['name'] == series_name), None)
        if not series_info:
            return []
        
        return [tip for tip in self.data['tips'] if tip['id'] in series_info['tip_ids']]


async def main():
    """Main function to generate example videos"""
    # Setup paths
    config_path = "/Users/nick/clawd/kiin-content/config/expanded_caregiver_tips.json"
    output_dir = "/Users/nick/clawd/kiin-content/output"
    
    # Create enhanced generator
    generator = TipsGeneratorV2(config_path, output_dir)
    
    print("ğŸš€ Kiin Caregiver Tips Generator V2 - Professional Educational Content")
    print("=" * 80)
    
    try:
        # Generate 3 example videos with different difficulties and series
        example_tips = [
            # Beginner tip from communication series
            next(tip for tip in generator.data['tips'] 
                 if tip.get('difficulty') == 'beginner' and tip.get('series') == 'communication_week'),
            
            # Intermediate tip from self-care series  
            next(tip for tip in generator.data['tips']
                 if tip.get('difficulty') == 'intermediate' and tip.get('series') == 'self_care_week'),
            
            # Advanced tip from emotional mastery series
            next(tip for tip in generator.data['tips']
                 if tip.get('difficulty') == 'advanced' and tip.get('series') == 'emotional_mastery')
        ]
        
        generated_videos = []
        
        for i, tip in enumerate(example_tips, 1):
            if tip:
                print(f"\nğŸ¬ Creating Example Video {i}/3")
                filename = f"tips_v2_example_{i}.mp4"
                video_path = await generator.generate_tip_video_v2(tip, filename)
                generated_videos.append(video_path)
        
        print(f"\nğŸ‰ Success! Generated {len(generated_videos)} example videos:")
        for video_path in generated_videos:
            print(f"   âœ… {video_path}")
        
        print(f"\nğŸ“Š Enhanced Features Delivered:")
        print(f"   ğŸ¨ Professional animated graphics and transitions")
        print(f"   ğŸµ Background music integration")
        print(f"   ğŸ“ˆ Data visualizations for impact")
        print(f"   ğŸ§  Memory aids and takeaways")
        print(f"   ğŸ¯ Action prompts for immediate implementation")
        print(f"   ğŸ·ï¸  Brand-consistent design")
        print(f"   ğŸ¤ Optimized voice selection")
        print(f"   ğŸ“± Progress indicators")
        print(f"   ğŸ“ Educational structure with multiple difficulty levels")
        
    except Exception as e:
        print(f"âŒ Error generating videos: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())